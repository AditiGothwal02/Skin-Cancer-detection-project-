{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditiGothwal02/Skin-Cancer-detection-project-/blob/main/spare_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYNScbObzFwi"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TJq87hY2zuzy",
        "outputId": "267e8ee4-0e72-4dfe-802d-3be152ab159c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/nightfury007/ham10000-isic2018-raw\n",
            "License(s): Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download nightfury007/ham10000-isic2018-raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DkiyKYdtz7SR"
      },
      "outputs": [],
      "source": [
        "!unzip ham10000-isic2018-raw.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zrAVCZxBugYy",
        "outputId": "66b073d0-2c67-4fde-e7cb-7b9bf38b3d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymMCvURL0BBr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set dataset directory\n",
        "dataset_dir = 'dataverse_files'\n",
        "\n",
        "# Load metadata\n",
        "metadata = pd.read_csv(os.path.join(dataset_dir, 'HAM10000_metadata'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s8m6AWRJjkNY",
        "outputId": "f96d1c7d-48ac-4793-a007-55265bcf132b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in metadata:\n",
            " lesion_id        0\n",
            "image_id         0\n",
            "dx               0\n",
            "dx_type          0\n",
            "age             57\n",
            "sex              0\n",
            "localization     0\n",
            "dataset          0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Missing values in metadata:\\n\", metadata.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aGV4hrTjri8"
      },
      "outputs": [],
      "source": [
        "metadata = metadata.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPqUpjlTPkcZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "roZjzN21j0uE",
        "outputId": "5a3f741b-fb74-45b4-f31e-2ed5ec3d4148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in metadata:\n",
            " lesion_id       0\n",
            "image_id        0\n",
            "dx              0\n",
            "dx_type         0\n",
            "age             0\n",
            "sex             0\n",
            "localization    0\n",
            "dataset         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Missing values in metadata:\\n\", metadata.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFewUwBjj4sD"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "metadata['encoded_labels'] = le.fit_transform(metadata['dx'])\n",
        "\n",
        "# Data cleaning: Remove duplicates\n",
        "metadata = metadata.drop_duplicates(subset='image_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtmCfd9XlTcc"
      },
      "outputs": [],
      "source": [
        "# Data balancing: Ensure each class has the same number of samples\n",
        "def balance_data(df, target_col, n_samples):\n",
        "    balanced_df = pd.DataFrame()\n",
        "    for label in df[target_col].unique():\n",
        "        label_df = df[df[target_col] == label]\n",
        "        if len(label_df) > n_samples:\n",
        "            label_df = label_df.sample(n_samples, random_state=42)\n",
        "        balanced_df = pd.concat([balanced_df, label_df])\n",
        "    return balanced_df\n",
        "\n",
        "# Determine the minimum number of samples in any class\n",
        "min_samples = metadata['encoded_labels'].value_counts().min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs5wiCqrlYmj"
      },
      "outputs": [],
      "source": [
        "# Balance the dataset\n",
        "metadata_balanced = balance_data(metadata, 'encoded_labels', min_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcU1JRFo0Ts2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define image size and batch size\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize ImageDataGenerator with data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(img_path, img_size):\n",
        "    img = load_img(img_path, target_size=img_size)  # Resize image\n",
        "    img_array = img_to_array(img)  # Convert to array\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions\n",
        "    img_array = preprocess_input(img_array)  # Normalize for VGG16\n",
        "    return img_array\n",
        "\n",
        "# Load images and labels\n",
        "images = []\n",
        "labels = []\n",
        "for index, row in metadata.iterrows():\n",
        "    img_path = os.path.join(dataset_dir, 'HAM10000_images_combined_600x450', row['image_id'] + '.jpg')\n",
        "    img_array = load_and_preprocess_image(img_path, img_size)\n",
        "    images.append(img_array)\n",
        "    labels.append(row['dx'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU71FiiptMTG"
      },
      "outputs": [],
      "source": [
        "# Convert lists to numpy arrays\n",
        "images = np.vstack(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "label_mapping = {label: idx for idx, label in enumerate(np.unique(labels))}\n",
        "labels_encoded = np.array([label_mapping[label] for label in labels])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL7JicTOtSSK"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels_encoded, test_size=0.2, random_state=42, stratify=labels_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUn48xTp1YtZ"
      },
      "outputs": [],
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "# Feature extraction using VGG16\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "def extract_features(model, data):\n",
        "    data = np.array(data)  # Ensure data is a NumPy array\n",
        "    features = model.predict(data, batch_size=32, verbose=1)  # Reduce batch size\n",
        "    return features\n",
        "\n",
        "# Extract features\n",
        "X_train_features = extract_features(vgg_model, X_train)\n",
        "X_test_features = extract_features(vgg_model, X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6LeEkZCs7y-"
      },
      "outputs": [],
      "source": [
        "# Compute class weights for balancing\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(labels_encoded),\n",
        "    y=labels_encoded\n",
        ")\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJn4mA7-s-8e"
      },
      "outputs": [],
      "source": [
        "# Train SVM classifier with class weights\n",
        "svm_classifier = SVC(kernel='linear', probability=True, class_weight=class_weights_dict)\n",
        "svm_classifier.fit(X_train_features, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V60f7n3tBbS"
      },
      "outputs": [],
      "source": [
        "# Predict on test data\n",
        "y_pred = svm_classifier.predict(X_test_features)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred, target_names=label_mapping.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5peFDBerHHuO"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL0GyRaB9Vmj"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "import joblib\n",
        "import pickle\n",
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "# Define save paths\n",
        "svm_model_path = 'svm_classifier.pkl'\n",
        "vgg_model_path = 'vgg16_model.h5'\n",
        "vgg_features_train_path = 'X_train_features.npy'\n",
        "vgg_features_test_path = 'X_test_features.npy'\n",
        "y_train_path = 'y_train.npy'\n",
        "y_test_path = 'y_test.npy'\n",
        "\n",
        "# Save the trained SVM model\n",
        "joblib.dump(svm_classifier, svm_model_path)\n",
        "print(f\"SVM model saved to {svm_model_path}\")\n",
        "\n",
        "# Save the VGG16 model\n",
        "save_model(vgg_model, vgg_model_path)\n",
        "print(f\"VGG16 model saved to {vgg_model_path}\")\n",
        "\n",
        "# Save extracted features and labels\n",
        "np.save(vgg_features_train_path, X_train_features)\n",
        "np.save(vgg_features_test_path, X_test_features)\n",
        "np.save(y_train_path, y_train)\n",
        "np.save(y_test_path, y_test)\n",
        "print(\"Feature arrays and labels saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaJfa5KH9oES"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "# Load the trained models\n",
        "svm_model_path = 'svm_classifier.pkl'\n",
        "vgg_model_path = 'vgg16_model.h5'\n",
        "svm_classifier = joblib.load(svm_model_path)\n",
        "vgg_model = load_model(vgg_model_path)\n",
        "\n",
        "# Define image size\n",
        "img_size = (224, 224)\n",
        "\n",
        "# Function to preprocess input image\n",
        "def preprocess_image(img_path):\n",
        "    img = load_img(img_path, target_size=img_size)  # Load and resize image\n",
        "    img_array = img_to_array(img)  # Convert to array\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions for model input\n",
        "    img_array = preprocess_input(img_array)  # Normalize\n",
        "    return img_array\n",
        "\n",
        "# Function to predict class\n",
        "def predict_image_class(img_path, label_mapping):\n",
        "    img_array = preprocess_image(img_path)\n",
        "    features = vgg_model.predict(img_array)  # Extract features using VGG16\n",
        "    prediction = svm_classifier.predict(features)  # Predict using SVM\n",
        "    class_label = list(label_mapping.keys())[list(label_mapping.values()).index(prediction[0])]\n",
        "    return class_label\n",
        "\n",
        "\n",
        "# Example usage\n",
        "img_path = 'ISIC_0024331.jpg'  # Provide path to the input image\n",
        "predicted_class = predict_image_class(img_path, label_mapping)\n",
        "print(f'Predicted Class: {predicted_class}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0F9CDNqN7AO"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STjTAFZxSanv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}